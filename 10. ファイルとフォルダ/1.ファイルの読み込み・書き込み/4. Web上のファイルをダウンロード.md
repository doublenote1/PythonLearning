# URLを指定してファイルをダウンロード

> 個別のファイルのURLを指定してダウンロードするのは
  標準ライブラリのみの使用でOK

## コード例

> URL、保存先ディレクトリ、オプションでファイル名を指定して
  ファイルをダウンロード・保存する関数とその使い方のコード例を示す

```python
import os
import urllib.error
import urllib.request
import shutil
import user

def download_file(url, dst_dir, dst_file_name=''):
    try:
        with urllib.request.urlopen(url) as web_file:
            data = web_file.read()
            if dst_file_name:
                path = os.path.join(dst_dir, dst_file_name)
            else:
                path = os.path.join(dst_dir, os.path.basename(url))
            if not os.path.isdir(dst_dir):
                os.makedirs(dst_dir)
            with open(path, mode='wb') as local_file:
                local_file.write(data)
    except urllib.error.URLError as e:
        print(e)

"<dst_file_name>のデフォルトはURLのファイル名"

root = 'temp'
git_hub = 'https://github.com/nkmk/python-snippets/raw/master/notebook/data/src/'

url_png = 'https://www.python.org/static/img/python-logo.png'
url_zip = git_hub + 'sample_header.csv.zip'
url_xlsx = git_hub + 'sample.xlsx'
url_pdf = git_hub + 'pdf/sample1.pdf'
download_file(url_png, root, 'py-logo.png')
download_file(url_png, root)
download_file(url_zip, root)
download_file(url_xlsx, root)
download_file(url_pdf, root)
user.show_entry('temp')

shutil.rmtree('temp')
os.mkdir('temp')
```

> `os.path.basename()`でURLからファイル名を抽出し、
  `os.path.join()`で指定したディレクトリと結合して
  保存先のパスを生成している

> パス文字列の操作についての詳細は以下の記事を参照。
[note.nkmk.me](https://note.nkmk.me/python-os-basename-dirname-split-splitext/)

> 以下、データの取得部分とファイルとして保存する部分について説明する

## URLを開く:urllib.request.urlopen()

`ファイルオブジェクト = 
urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`

> `urllib.request.urlopen()`でURLを開きデータを取得する

> 例外が発生しても止まらないように、`try`, `except`でエラーを捕捉する
> 例では`urllib.error`をインポートして、`urllib.error.URLError`のみを
  明示的に捕捉している
> ファイルのURLが存在しない場合などにエラーメッセージが表示される。
[docs.python.org](https://docs.python.org/ja/3/library/urllib.error.html)

> 標準ライブラリの`urllib`ではなく
  サードパーティライブラリの`Requests`を使って`url`を開いて
  データを取得することも可能。
[note.nkmk.me](https://note.nkmk.me/python-requests-usage/)

> ユーザーエージェントなどリクエストヘッダの変更・追加は
  `Requests`を使うと簡単。
[note.nkmk.me](https://note.nkmk.me/python-requests-usage/)

## バイナリモードでファイルに書き込み:open()

> `urllib.request.urlopen()`で取得できるデータはバイト列(bytes型)。
> それをそのまま第二引数`mode='wb'`とした`open()`で
  バイナリとして書き込む
> `w`が書き込み、`b`がバイナリの意味。
[関連記事](1. ファイル作成・読み書き.md)

## よりシンプルなコード例

> 省略

> なお、この関数で指定するURLはファイルそのものへのリンク
  でなければならない

# WebページのファイルのURLを抽出

> ページ内の画像を一括でダウンロードするには、
  まず画像のURLを抽出してリストを作成する

## 連番になっている場合

> ダウンロードしたい画像のURLが単純な連番になっている場合は簡単
> 連番に限らず何らかの規則性があれば、
  後述の`Beautiful Soup`などでスクレイピングをするより、
  規則に従ってURLのリストを作ってしまったほうが楽。

[1](../../5.%20基本データ型/3.%20文字列型/7.%20書式変換/1.%20format()関数・メソッドで書式変換.md#ゼロ埋め)
```python
import pprint
"[1] format()でゼロ埋め"
url_list = ['https://example.com/basedir/base_{:03}.jpg'.format(i) for i in range(5)]
pprint.pprint(url_list)
# -> ['https://example.com/basedir/base_000.jpg',
# ->  'https://example.com/basedir/base_001.jpg',
# ->  'https://example.com/basedir/base_002.jpg',
# ->  'https://example.com/basedir/base_003.jpg',
# ->  'https://example.com/basedir/base_004.jpg']
```

## Beautiful Soupで抽出(未検証)

> Webページの画像URLを一括で抽出するにはBeautiful Soupを使う。

```python
import urllib.request
from bs4 import BeautifulSoup

url = 'https://news.yahoo.co.jp/list/'
ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) '\
     'AppleWebKit/537.36 (KHTML, like Gecko) '\
     'Chrome/55.0.2883.95 Safari/537.36 '

req = urllib.request.Request(url, headers={'User-Agent': ua})
html = urllib.request.urlopen(req)

soup = BeautifulSoup(html, "html.parser")

url_list = [img.get('data-src') for img in soup.find(class_='list').find_all('img')]
print(url_list)
```

> 例ではYahoo!ニュースのトピックスのサムネイル画像のURLを抽出している。

> Webページによって構成は異なるが、基本的には、
    * ダウンロードしたい複数の画像が含まれるブロックの
      classやidなどを指定して、<img>タグのオブジェクトのリストを取得
        soup.find(class_='list').find_all('img')の部分
    * <img>タグのsrc要素やdata-src要素から画像のURLを取得
        img.get('data-src')
という流れになる。

> なお、上のサンプルコードはあくまでも一例であり動作を保証するものではない
> Yahoo!のページ構成が変更されると動かなくなる可能性がある。

> Beautiful Soupについては以下の記事も参照。
[note.nkmk.me](https://note.nkmk.me/python-beautiful-soup-scraping-yahoo/)

## URLのリストから複数画像を一括ダウンロード(未検証)

> URLのリストがあれば、forループで回して、
  最初に示したURLを指定してファイルをダウンロード・保存する関数を
  呼び出すだけ
> 仮のURLリストのため、ここでは関数呼び出し`download_image_dir()`は
  コメントアウトしている。

```python
import time
download_dir = 'data/temp'
sleep_time_sec = 1

for url in url_list:
    print(url)
#     download_file_dir(url, download_dir)
    time.sleep(sleep_time_sec)
# https://example.com/basedir/base_000.jpg
# https://example.com/basedir/base_001.jpg
# https://example.com/basedir/base_002.jpg
# https://example.com/basedir/base_003.jpg
# https://example.com/basedir/base_004.jpg
```

> サーバーに負担をかけないように、画像を1枚ダウンロードするごとに
  `time.sleep()`で待機時間をつくっている
> 単位は秒なので、上の例では1秒ずつスリープ
> timeモジュールをimportして使う。

> 例は画像ファイルの場合だが、
  その他の種類のファイルでもリストになっていれば
  同様にまとめてダウンロード可能。

# リンク

[note.nkmk.me](https://note.nkmk.me/python-download-web-images/)
